---
sidebar_label: 'Streaming Utilities'
title: 'Streaming Utilities API Reference'
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Agent Forge provides utilities to help visualize the real-time flow of information within and between agents, especially when streaming LLM responses or observing multi-step processes.

## `enableConsoleStreaming()` Function

This function activates a console-based logger that subscribes to various internal Agent Forge events and prints formatted information to the console. It helps in observing:

*   Communication between agents.
*   LLM response chunks as they arrive.
*   Completion of LLM streams.
*   Progress through workflow steps.
*   Completion of tasks within teams.
*   Overall execution completion.

```typescript
export function enableConsoleStreaming(): void
```

**Usage:**

Call this function once in your application, typically during setup, if you want to see detailed real-time logging of agent activities.

```typescript
import { enableConsoleStreaming, AgentForge, LLM, Agent } from "agent-forge"; // Assuming exports

// Enable console streaming at the start of your application
enableConsoleStreaming();

async function main() {
  // ... (your Agent Forge setup: LLM, AgentForge instance, Agent registration) ...

  // const llm = new LLM("openai", { apiKey: process.env.OPENAI_API_KEY });
  // const forge = new AgentForge(llm);
  // const myAgent = new Agent({ name: "MyStreamAgent", /*...config...*/ model: "gpt-3.5-turbo" }, [], llm);
  // forge.registerAgent(myAgent);

  // When you run an agent with streaming enabled in its options,
  // or when agents communicate, you will see output in the console.
  // Example (conceptual):
  // await forge.runAgent("MyStreamAgent", "Tell me a short story.", { stream: true });
  
  // Example of Agent Communication log:
  // <agents>ManagerAgent â†’ ResearcherAgent:</agents>
  // Please research the latest trends in AI.

  // Example of LLM Stream log:
  // <agents>ResearcherAgent:</agents>
  // Okay, I will start researching the latest trends in AI. One moment...
  // [Chunk 1] According to recent articles...
  // [Chunk 2] ...several breakthroughs have occurred...
  // [Chunk 3] ...particularly in generative models.
  // (newline after stream completion)
}

// main();
```

**Details:**

*   The function registers listeners on the `globalEventEmitter` for events defined in `AgentForgeEvents`.
*   It formats messages to distinguish between agent-to-agent communication and LLM responses.
*   It handles the progressive output of LLM stream chunks.
*   If called multiple times, it clears previous listeners to prevent duplicate logging.
*   It uses a simple deduplication cache for agent communication messages to reduce noise from potentially redundant internal events.

This utility is primarily for debugging and observation purposes during development. 