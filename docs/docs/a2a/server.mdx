---
title: A2A Server Setup
sidebar_label: Server (A2AServer)
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Hosting an Agent with `A2AServer`

The `A2AServer` class in Agent Forge (`src/a2a/server/A2AServer.ts`) allows you to take any existing Agent Forge `Agent` and expose its capabilities over the network using the A2A (Agent-to-Agent) communication protocol. This turns your agent into a service that other agents or applications can interact with remotely.

## Core Functionality

-   **HTTP Server**: Creates and manages an HTTP server to listen for incoming A2A requests.
-   **Agent Hosting**: Takes an instance of an Agent Forge `Agent` that will handle the incoming tasks.
-   **Task Handling**: Manages the lifecycle of tasks sent by A2A clients, including receiving tasks, executing them using the hosted agent, and making results available.
-   **JSON-RPC Endpoint**: Implements the A2A JSON-RPC methods (`agent/getCard`, `tasks/send`, `tasks/get`, `tasks/cancel`, `tasks/sendSubscribe`).
-   **Agent Card**: Automatically serves an "Agent Card" (`/.well-known/agent.json`), which provides metadata about the hosted agent (name, description) to potential clients.
-   **Streaming (SSE)**: Supports Server-Sent Events for the `tasks/sendSubscribe` method, allowing clients to receive real-time updates on task progress, including LLM stream chunks if the underlying agent and LLM support streaming.

## `A2AServerOptions`

When creating an `A2AServer`, you provide server options through the `A2AServerOptions` interface (`src/a2a/server/types.ts`):

```typescript
export interface A2AServerOptions {
  port: number;                 // Port for the HTTP server to listen on (e.g., 41241)
  host: string;                 // Hostname for the server (e.g., 'localhost', '0.0.0.0')
  endpoint: string;             // Path for the A2A JSON-RPC endpoint (e.g., '/a2a')
  agentCardPath?: string;       // Optional: Path for the agent card (defaults to '/.well-known/agent.json')
  useStreaming?: boolean;       // Optional: Enable SSE streaming for tasks/sendSubscribe (defaults to true)
  customHttpServer?: any;       // Optional: Provide your own Node.js http.Server instance
  logger?: any;                 // Optional: Custom logger instance
}
```

## `A2ATaskHandler` and `AgentToTaskHandlerAdapter`

Internally, `A2AServer` uses an `A2ATaskHandler` to process tasks. This is an async generator function that yields status updates and artifacts.

```typescript
// src/a2a/server/types.ts
export type A2ATaskHandler = (
  taskId: string,
  message: A2AMessage
) => AsyncGenerator<A2ATaskUpdateEvent, void, unknown>;
```

Agent Forge provides a `defaultAgentToTaskHandlerAdapter` (`src/a2a/server/agentAdapter.ts`) that converts a standard Agent Forge `Agent` into an `A2ATaskHandler`. This adapter:

-   Calls the agent's `run()` method.
-   Listens to `LLM_STREAM_CHUNK` events from the `globalEventEmitter` to provide real-time streaming updates.
-   Translates the agent's execution (working, completed, failed, tool calls) into A2A task status updates and artifacts.

You can provide your own adapter if you need custom logic for how an agent's execution translates to A2A events.

## Setting Up an `A2AServer`

Here's a conceptual example of how to set up an `A2AServer`:

```typescript
import { Agent } from "agent-forge"; // Adjust path as necessary
import { LLM } from "agent-forge";   // Adjust path
import {
  A2AServer,
  defaultAgentToTaskHandlerAdapter,
  type A2AServerOptions,
} from "agent-forge/a2a"; // Adjust path for A2A imports

async function startMyAgentServer() {
  // 1. Define your Agent Forge agent
  const agentConfig = {
    name: "MyHostedAgent",
    role: "Service Provider",
    description: "An agent hosted via A2A to perform specific tasks.",
    objective: "To fulfill remote requests accurately.",
    model: "gpt-3.5-turbo", // Or your preferred model
  };

  // Ensure you have your LLM provider configured (e.g., OpenAI API Key)
  const llmProvider = new LLM("openai", { apiKey: process.env.OPENAI_API_KEY });
  const myAgent = new Agent(agentConfig, [], llmProvider);

  // 2. Configure the A2AServer
  const serverOptions: A2AServerOptions = {
    port: 41241,
    host: "localhost",
    endpoint: "/myagent-a2a",
    useStreaming: true,
  };

  // 3. Create the A2AServer instance
  const a2aServer = new A2AServer(
    myAgent,                            // The agent to host
    serverOptions,                      // Server configuration
    defaultAgentToTaskHandlerAdapter  // The adapter to bridge agent logic with A2A protocol
  );

  // 4. Start the server
  try {
    await a2aServer.start();
    console.log(
      `A2A Server for ${myAgent.name} is running on http://${serverOptions.host}:${serverOptions.port}${serverOptions.endpoint}`
    );
    console.log(
      `Agent Card: http://${serverOptions.host}:${serverOptions.port}/.well-known/agent.json`
    );
  } catch (error) {
    console.error("Failed to start A2A server:", error);
  }
}

startMyAgentServer();
```

**To run this example:**

1.  Replace placeholder import paths with actual paths relevant to your project structure.
2.  Ensure `OPENAI_API_KEY` (or your chosen LLM's API key) is set in your environment.
3.  Compile and run the script.

Once running, your `MyHostedAgent` will be accessible to A2A clients at the specified host, port, and endpoint.

### Stopping the Server

The `A2AServer` instance has a `stop()` method that will gracefully shut down the HTTP server:

```typescript
// await a2aServer.stop();
```

See the `a2a-server-example.ts` in the `src/examples/` directory of the Agent Forge repository for a runnable example. 